# Ultralytics YOLO ðŸš€, AGPL-3.0 license
"""
Ultralytics modules.

Example:
    Visualize a module with Netron.
    ```python
    from ultralytics.nn.modules import *
    import torch
    import os

    x = torch.ones(1, 128, 40, 40)
    m = Conv(128, 128)
    f = f"{m._get_name()}.onnx"
    torch.onnx.export(m, x, f)
    os.system(f"onnxslim {f} {f} && open {f}")  # pip install onnxslim
    ```
"""

from .block import (
    C1,
    C2,
    C2PSA,
    C2PSADyT,
    C3,
    C3TR,
    CIB,
    DFL,
    ELAN1,
    PSA,
    SPP,
    SPPELAN,
    SPPF,
    AConv,
    ADown,
    Attention,
    BNContrastiveHead,
    Bottleneck,
    BottleneckCSP,
    C2f,
    C2fAttn,
    C2fCIB,
    C2fPSA,
    C3Ghost,
    C3k2,
    C3x,
    CBFuse,
    CBLinear,
    ContrastiveHead,
    GhostBottleneck,
    HGBlock,
    HGStem,
    ImagePoolingAttn,
    Proto,
    RepC3,
    RepNCSPELAN4,
    RepVGGDW,
    ResNetLayer,
    SCDown,
    CARAFE,
    
    EC2f,
    EMA,
    EC3k2,
    C3k2_DCNv2,

    C2AIFI,
    C2AIFIDyT,
    C2CA,
    C2EMA,
    C2GAM,
    C2LSKA,
    C2SE,
    C2TripletAttention,
    C2CAA,
    C2ECA,
    C2CBAM,

    FasterNetBlock,
    PartialConv3d,
    FC3,
    FC3k,
    FC2f,
    FC3k2,
    EFC2f,
    EFC3k2,
    C3k2_Faster,
    C3k2_Faster_EMA,
    C3k2_Faster_CAA,
    C3k2_Faster_CGLU,
    C3k2_Star,
    C3k2_Star_EMA,
    C3k2_Star_CAA,
    C3k2_CaFormerCGLU,

    DyHeadBlock, 

    GSConv, 
    GSConvns, 
    VoVGSCSP, 
    VoVGSCSPns, 
    VoVGSCSPC,

    EMA,
    CoordAtt,
    GAM,
    LSKA,
    SEAttention,
    TripletAttention,
    CAA,
    EfficientChannelAttention,
)
from .conv import (
    CBAM,
    ChannelAttention,
    Concat,
    Conv,
    Conv2,
    ConvTranspose,
    DWConv,
    DWConvTranspose2d,
    Focus,
    GhostConv,
    LightConv,
    RepConv,
    SpatialAttention,
)
from .head import OBB, Classify, Detect, Pose, RTDETRDecoder, Segment, WorldDetect, v10Detect, Detect_DyHead, Detect_LSCD
from .transformer import (
    AIFI,
    MLP,
    DeformableTransformerDecoder,
    DeformableTransformerDecoderLayer,
    LayerNorm2d,
    MLPBlock,
    MSDeformAttn,
    TransformerBlock,
    TransformerEncoderLayer,
    TransformerLayer,
)

__all__ = (
    "Conv",
    "Conv2",
    "LightConv",
    "RepConv",
    "DWConv",
    "DWConvTranspose2d",
    "ConvTranspose",
    "Focus",
    "GhostConv",
    "ChannelAttention",
    "SpatialAttention",
    "CBAM",
    "Concat",
    "TransformerLayer",
    "TransformerBlock",
    "MLPBlock",
    "LayerNorm2d",
    "DFL",
    "HGBlock",
    "HGStem",
    "SPP",
    "SPPF",
    "C1",
    "C2",
    "C3",
    "C2f",
    "C3k2",
    "SCDown",
    "C2fPSA",
    "C2PSA",
    "C2PSADyT",
    "C2fAttn",
    "C3x",
    "C3TR",
    "C3Ghost",
    "GhostBottleneck",
    "Bottleneck",
    "BottleneckCSP",
    "Proto",
    "Detect",
    "Segment",
    "Pose",
    "Classify",
    "TransformerEncoderLayer",
    "RepC3",
    "RTDETRDecoder",
    "AIFI",
    "DeformableTransformerDecoder",
    "DeformableTransformerDecoderLayer",
    "MSDeformAttn",
    "MLP",
    "ResNetLayer",
    "OBB",
    "WorldDetect",
    "v10Detect",
    "Detect_LSCD",
    "Detect_TADDH",
    "ImagePoolingAttn",
    "ContrastiveHead",
    "BNContrastiveHead",
    "RepNCSPELAN4",
    "ADown",
    "SPPELAN",
    "CBFuse",
    "CBLinear",
    "AConv",
    "ELAN1",
    "RepVGGDW",
    "CIB",
    "C2fCIB",
    "Attention",
    "PSA",
    "CARAFE",
    
    "EC2f",
    "EC3k2",
    "C3k2_DCNv2",

    "C2CBAM",
    "C2AIFI",
    "C2AIFIDyT",
    "C2CA",
    "C2EMA",
    "C2GAM",
    "C2LSKA",
    "C2SE",
    "C2TripletAttention",
    "C2CAA",
    "C2ECA",
    "C2CBAM",

    "PartialConv3d",
    "FasterNetBlock",
    "FC3",
    "FC3k",
    "FC2f",
    "FC3k2",
    "EFC2f",
    "EFC3k2",
    "C3k2_Faster",
    "C3k2_Faster_EMA",
    "C3k2_Faster_CAA",
    "C3k2_Faster_CGLU",
    "C3k2_Star",
    "C3k2_Star_EMA",
    "C3k2_Star_CAA",
    "C3k2_CaFormerCGLU",

    'DyHeadBlock', 

    "GSConv", 
    "GSConvns", 
    "VoVGSCSP", 
    "VoVGSCSPns", 
    "VoVGSCSPC",

    "EMA",
    "CoordAtt",
    "GAM",
    "LSKA",
    "SEAttention",
    "TripletAttention",
    "CAA",
    "EfficientChannelAttention",
)

